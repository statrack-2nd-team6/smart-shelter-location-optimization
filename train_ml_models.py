#!/usr/bin/env python3
"""
ìŠ¤ë§ˆíŠ¸ ì‰¼í„° ìš°ì„ ìˆœìœ„ ì˜ˆì¸¡ - ML ëª¨ë¸ í•™ìŠµ
- ë°°í¬(Streamlit Cloud) ì•ˆì •ì„±ì„ ìœ„í•´ pickle ëŒ€ì‹  joblib ì‚¬ìš©
- ì‹¤í–‰ ìœ„ì¹˜(cwd)ì™€ ë¬´ê´€í•˜ê²Œ ë™ì‘í•˜ë„ë¡ ê²½ë¡œë¥¼ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •
"""

from __future__ import annotations

import csv
import json
from pathlib import Path

import numpy as np
from joblib import dump
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# -----------------------------------------------------------------------------
# Paths
# -----------------------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent
DATASET_PATH = BASE_DIR / "dataset_engineered.csv"

SCALER_PATH = BASE_DIR / "scaler.joblib"
BEST_MODEL_PATH = BASE_DIR / "best_model.joblib"
METADATA_PATH = BASE_DIR / "model_metadata.json"
SEOUL_DATA_PATH = BASE_DIR / "seoul_data.json"


def _rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))


def main() -> None:
    print("=" * 80)
    print("ğŸ¤– ML ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 80)
    print()

    # -------------------------------------------------------------------------
    # Load data
    # -------------------------------------------------------------------------
    print("ğŸ“¥ ë°ì´í„° ë¡œë”©...")

    if not DATASET_PATH.exists():
        raise FileNotFoundError(
            f"dataset_engineered.csvë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
            f"- ê¸°ëŒ€ ê²½ë¡œ: {DATASET_PATH}\n"
            f"- í˜„ì¬ íŒŒì¼ ê¸°ì¤€ í´ë”(BASE_DIR): {BASE_DIR}"
        )

    with open(DATASET_PATH, "r", encoding="utf-8-sig", newline="") as f:
        reader = csv.DictReader(f)
        data = list(reader)

    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data):,}ê°œ ìƒ˜í”Œ")
    print()

    # -------------------------------------------------------------------------
    # Prepare features and target
    # -------------------------------------------------------------------------
    print("ğŸ”§ Feature ë° Target ì¤€ë¹„...")

    feature_cols = [
        "cai", "pm25", "pm10", "o3", "no2",
        "ridership", "ridership_log",
        "dispatch_interval", "dispatch_half",
        "lat", "lon",
        "cai_ridership", "pollution_exposure_v2", "total_exposure",
        "is_high_traffic", "is_high_pollution", "is_long_wait",
    ]

    X: list[list[float]] = []
    y: list[float] = []
    seoul_rows = []  # For web app (seoul_data.json)

    missing_cols = set()
    for row in data:
        for col in feature_cols + ["priority_v4", "stop_id", "name", "district", "lat", "lon", "cai", "ridership"]:
            if col not in row:
                missing_cols.add(col)

    if missing_cols:
        raise KeyError(f"CSVì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {sorted(missing_cols)}")

    for row in data:
        features = [float(row[col]) for col in feature_cols]
        target = float(row["priority_v4"])

        X.append(features)
        y.append(target)

        # Save metadata for web app (ì„œìš¸ ì •ë¥˜ì¥ ë¦¬ìŠ¤íŠ¸)
        seoul_rows.append({
            "stop_id": row["stop_id"],
            "name": row["name"],
            "district": row["district"],
            "lat": float(row["lat"]),
            "lon": float(row["lon"]),
            "cai": float(row["cai"]),
            "ridership": int(float(row["ridership"])),
            "priority": target,
        })

    X_np = np.array(X, dtype=np.float64)
    y_np = np.array(y, dtype=np.float64)

    print(f"âœ… Feature shape: {X_np.shape}")
    print(f"âœ… Target shape: {y_np.shape}")
    print(f"âœ… Feature ëª©ë¡ ({len(feature_cols)}ê°œ):")
    for i, col in enumerate(feature_cols, 1):
        print(f"   {i:2d}. {col}")
    print()

    # -------------------------------------------------------------------------
    # Train-test split
    # -------------------------------------------------------------------------
    print("âœ‚ï¸  Train-Test Split (80:20)...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_np, y_np, test_size=0.2, random_state=42
    )
    print(f"âœ… Train: {len(X_train):,}ê°œ")
    print(f"âœ… Test: {len(X_test):,}ê°œ")
    print()

    # -------------------------------------------------------------------------
    # Feature scaling (IMPORTANT: keep consistent with web app)
    # -------------------------------------------------------------------------
    print("ğŸ“Š Feature Scaling (StandardScaler)...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    print("âœ… Scaling ì™„ë£Œ")
    print()

    # -------------------------------------------------------------------------
    # Train models
    # -------------------------------------------------------------------------
    print("=" * 80)
    print("ğŸ¯ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 80)
    print()

    # NOTE:
    # Streamlit appì—ì„œ scaler.transform(features)ë¥¼ í•œ ë’¤ model.predict()ë¥¼ í˜¸ì¶œí•˜ê³  ìˆìœ¼ë¯€ë¡œ,
    # ëª¨ë“  ëª¨ë¸ì„ "scaled ì…ë ¥" ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµ/ì˜ˆì¸¡í•˜ë„ë¡ í†µì¼.
    models = {
        "Linear Regression": LinearRegression(),
        "Ridge Regression": Ridge(alpha=1.0),
        "Random Forest": RandomForestRegressor(
            n_estimators=300,
            max_depth=20,
            min_samples_split=5,
            random_state=42,
            n_jobs=-1,
        ),
        "Gradient Boosting": GradientBoostingRegressor(
            n_estimators=300,
            max_depth=5,
            learning_rate=0.05,
            random_state=42,
        ),
    }

    results: dict[str, dict[str, float]] = {}

    for name, model in models.items():
        print(f"ğŸ”„ Training {name}...")

        model.fit(X_train_scaled, y_train)

        y_pred_train = model.predict(X_train_scaled)
        y_pred_test = model.predict(X_test_scaled)

        train_rmse = _rmse(y_train, y_pred_train)
        train_mae = float(mean_absolute_error(y_train, y_pred_train))
        train_r2 = float(r2_score(y_train, y_pred_train))

        test_rmse = _rmse(y_test, y_pred_test)
        test_mae = float(mean_absolute_error(y_test, y_pred_test))
        test_r2 = float(r2_score(y_test, y_pred_test))

        results[name] = {
            "train_rmse": train_rmse,
            "train_mae": train_mae,
            "train_r2": train_r2,
            "test_rmse": test_rmse,
            "test_mae": test_mae,
            "test_r2": test_r2,
        }

        print(f"   Train - RMSE: {train_rmse:.6f}, MAE: {train_mae:.6f}, RÂ²: {train_r2:.6f}")
        print(f"   Test  - RMSE: {test_rmse:.6f}, MAE: {test_mae:.6f}, RÂ²: {test_r2:.6f}")
        print()

    # -------------------------------------------------------------------------
    # Results summary
    # -------------------------------------------------------------------------
    print("=" * 80)
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    print("=" * 80)
    print()
    print(f'{"Model":<25s} {"Test RMSE":<12s} {"Test MAE":<12s} {"Test RÂ²":<12s}')
    print("-" * 80)
    for name, metrics in results.items():
        print(
            f'{name:<25s} '
            f'{metrics["test_rmse"]:<12.6f} '
            f'{metrics["test_mae"]:<12.6f} '
            f'{metrics["test_r2"]:<12.6f}'
        )
    print()

    best_model_name = max(results, key=lambda x: results[x]["test_r2"])
    best_model = models[best_model_name]

    print(f"ğŸ† Best Model: {best_model_name}")
    print(f'   Test RÂ² Score: {results[best_model_name]["test_r2"]:.6f}')
    print()

    # -------------------------------------------------------------------------
    # Feature importance (tree models)
    # -------------------------------------------------------------------------
    if best_model_name in ["Random Forest", "Gradient Boosting"]:
        print("=" * 80)
        print("ğŸ“Š Feature Importance (Top 10)")
        print("=" * 80)

        importances = best_model.feature_importances_
        feature_importance = sorted(
            zip(feature_cols, importances),
            key=lambda x: x[1],
            reverse=True,
        )

        for i, (feat, imp) in enumerate(feature_importance[:10], 1):
            print(f"{i:2d}. {feat:<30s}: {imp:.6f}")
        print()

    # -------------------------------------------------------------------------
    # Save artifacts (joblib + json)
    # -------------------------------------------------------------------------
    print("=" * 80)
    print("ğŸ’¾ ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ì €ì¥")
    print("=" * 80)

    # Scaler
    dump(scaler, SCALER_PATH)
    print(f"âœ… Scaler ì €ì¥: {SCALER_PATH.name}")

    # Best model
    dump(best_model, BEST_MODEL_PATH)
    print(f"âœ… Best Model ì €ì¥: {BEST_MODEL_PATH.name} ({best_model_name})")

    # All models (optional)
    for name, model in models.items():
        safe_name = name.replace(" ", "_").lower()
        path = BASE_DIR / f"model_{safe_name}.joblib"
        dump(model, path)
        print(f"âœ… {name} ì €ì¥: {path.name}")

    # Metadata for app
    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(
            {
                "feature_cols": feature_cols,
                "best_model_name": best_model_name,
                "results": results,
                "data_info": {
                    "total_samples": int(len(data)),
                    "train_samples": int(len(X_train)),
                    "test_samples": int(len(X_test)),
                    "n_features": int(len(feature_cols)),
                },
                # ì¤‘ìš”: ì•±ì—ì„œ ìŠ¤ì¼€ì¼ë§ í›„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë„ scaledë¡œ í†µì¼í–ˆìŒì„ ëª…ì‹œ
                "note": "All models were trained on StandardScaler-transformed features for deployment consistency.",
            },
            f,
            indent=2,
            ensure_ascii=False,
        )
    print(f"âœ… Metadata ì €ì¥: {METADATA_PATH.name}")

    # Seoul data for map/dashboard
    with open(SEOUL_DATA_PATH, "w", encoding="utf-8") as f:
        json.dump(seoul_rows, f, ensure_ascii=False)
    print(f"âœ… Seoul data ì €ì¥: {SEOUL_DATA_PATH.name}")

    print()
    print("=" * 80)
    print("âœ… ML ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    main()